---
title: "Caret Pre-Processing"
author: "Gergely Horvath"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: inline
---

Goal

It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. 


Metric

Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted_pls value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)

# Load Libraries
```{r}
library(earth)
library(shiny)
library(miniUI)
library(ggplot2)
library(data.table)
library(caret)
library(doParallel)
library(outliers)
library(pls)
library(broom)
library(scales)
```

# Set Up Shiny Gadget
```{r}
ggbrush <- function(data, xvar, yvar) {

  ui <- miniPage(
    gadgetTitleBar("Drag to select points"),
    miniContentPanel(
      # The brush="brush" argument means we can listen for
      # brush events on the plot using input$brush.
      plotOutput("plot", height = "100%", brush = "brush")
    )
  )

  server <- function(input, output, session) {

    # Render the plot
    output$plot <- renderPlot({
      # Plot the data with x/y vars indicated by the caller.
      ggplot(data, aes_string(xvar, yvar)) + geom_point()
    })

    # Handle the Done button being pressed.
    observeEvent(input$done, {
      # Return the brushed points. See ?shiny::brushedPoints.
      stopApp(brushedPoints(data, input$brush))
    })
  }

  runGadget(ui, server)
}
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE)
```


# Import Data

```{r}
data_descr <- read.csv(file = "data/data_description.txt")
sample_submission <- read.csv(file = "data/sample_submission.csv")
ds_test <- read.csv(file = "data/test.csv")
ds_test <- as.data.table(ds_test)
ds_train <- read.csv(file = "data/train.csv")
ds_train <- as.data.table(ds_train)

## Remove Predictors with too many NAs ========================================
# determine which variables should be kept, nr. of NAs low
var_na <- ds_train[, colMeans(is.na(ds_train)) < .95]
# show variables which will be removed
ds_train[, -..var_na]
# remove NA vars
ds_train <- ds_train[, ..var_na]
remove(var_na)

```


## Remove nearZeroVar
```{r}
near_zero_vars <- nearZeroVar(ds_train,freqCut = 95/5)
ds_train[, ..near_zero_vars]
ds_train <- ds_train[, -c(..near_zero_vars)]
remove(near_zero_vars)
```


## Reduce Collinearity
Correlation matrix:
```{r}
# get numeric columns
charidx <- unname(sapply(ds_train, function(x) is.numeric(x)))
dt_nr <- ds_train[, ..charidx]
# exclude SalePrice as we only want to analyze the correlation of predictors
dt_nr <- dt_nr[, -c("SalePrice")]
remove(charidx)
correlations <- cor(dt_nr, use = "complete.obs")
corrplot::corrplot(correlations, order = "hclust",tl.cex = 0.5)
```

Identify high correlation variables:
```{r}
highCorr <- findCorrelation(correlations, cutoff = 0.75, names = T)
head(ds_train[, ..highCorr])
```

Remove high correlation variables:

```{r}
ds_train <- ds_train[, -(..highCorr)]
remove(highCorr)
remove(correlations)
```

### Linear Dependencies
```{r}
combo_info <- findLinearCombos(dt_nr[complete.cases(dt_nr)])
combo_info
remove(combo_info)
remove(dt_nr)
```






##  bind train & test

```{r}
ds_train$status <- "train"
vars <- names(ds_test)[names(ds_test) %in% names(ds_train)]
ds_test <- ds_test[, ..vars]
ds_test$SalePrice <- NA
ds_test$status <- "test"
setdiff(names(ds_train), names(ds_test))
ds_all <- rbind(ds_train, ds_test)
remove(ds_train)
remove(ds_test)
```


# Replace NAs with 0
```{r}
ds_all[is.na(ds_all)] <- 0
```



# Dummy Variables

```{r}
ds_all_dummies <- predict(dummyVars(" ~ .", data = ds_all), newdata = ds_all)
ds_all_dummies <- as.data.table(ds_all_dummies)

# remove dummy because it does not exist in the test set
# ds_train_dummies$HouseStyle2.5Fin <- NULL
# ds_train_dummies$GarageQualEx <- NULL
# ds_train_dummies$Id <- NULL
```


## Impute NAs & Preprocess

```{r}
ds_train_transf <-
  preProcess(ds_all_dummies[ds_all_dummies$statustrain == 1, -c("statustest", "statustrain")],
             method = c("BoxCox", "center", "scale"))
# apply the preprocessing transformation
ds_all_dummies_prpr <- predict(ds_train_transf, ds_all_dummies)
ds_all_dummies_prpr$SalePrice <- ds_all_dummies$SalePrice
remove(ds_train_transf)
head(ds_all_dummies_prpr)
```


## Split Training & Test Set

```{r}
ds_all_dummies_prpr_train <- ds_all_dummies_prpr[statustrain == 1, -c("statustrain", "statustest")]
ds_all_dummies_prpr_test <- ds_all_dummies_prpr[statustest == 1, -c("SalePrice", "statustrain", "statustest")]
```


# TRAIN MODELS

## PLS

### Train Model PLS

```{r}
model_pls <-
  plsr(
    formula = SalePrice ~ .,
    data = ds_all_dummies_prpr_train,
    validation = "CV",
    ncomp = 4
  )

summary(model_pls)
```


RMSE is 24,679 (see line 'CV')
Variance explained by model (see line 'X'): 33.57%?



### Plot Model PLS

```{r}
plot(model_pls)
```


### Validation Plot

```{r}
validationplot(model_pls)
```

MSEP plot:
```{r}
validationplot(model_pls, val.type="MSEP")
```


R2 plot:
```{r}
validationplot(model_pls, val.type="R2")
```


### Prediction & ggbrush() of Training Set

```{r}
pred_train_pls <- predict(model_pls, ds_all_dummies_prpr_train, ncomp = 4)
# shiny
# ggbrush(ds_train_dummies, "SalePrice", "pred_train_pls")
```


### Plot Observed vs Prediction

```{r}
plot(x = ds_all_dummies_prpr_train$SalePrice, y = pred_train_pls)
abline(0,1)
```


### Plot Prediction vs Residuals

```{r}
plot(x = model_pls$fitted.values, y = model_pls$residuals)
```


### RMSE

```{r}
rmse_pls <- round(sqrt(mean((pred_train_pls - ds_all_dummies_prpr_train$SalePrice)^2)), 0)
rmse_pls
```

23919

### Prediction Test Set

```{r}
pred_test_pls <- predict(model_pls, ds_all_dummies_prpr_test, ncomp = 4)
head(pred_test_pls)
```


## MARS


### Train Model MARS (earth)

```{r}
model_mars_earth <- earth(ds_all_dummies_prpr_train[, -c("SalePrice")], ds_all_dummies_prpr_train$SalePrice)
# predict train
pred_train_mars_earth <- predict(model_mars_earth, ds_all_dummies_prpr_train[, -c("SalePrice")])
pred_train_mars_earth <- as.vector(pred_train_mars_earth)
model_mars_earth
```

### Summary MARS (earth)
```{r}
summary(model_mars_earth)
```

### Plot MARS (earth)
```{r}
plot(model_mars_earth)
```

### Plot Observed vs Prediction MARS (earth)
```{r}
plot(x = I(ds_all_dummies_prpr_train$SalePrice), y = ds_all_dummies_prpr_train$predicted_mars)
abline(0,1)
```


### Plot Prediction vs Residuals

```{r}
plot(x = model_mars_earth$fitted.values, y = model_mars_earth$residuals)
```


### RMSE

```{r}
rmse_mars_earth <- round(sqrt(mean((pred_train_mars_earth - ds_all_dummies_prpr_train$SalePrice)^2)), 0)
rmse_mars_earth
```

### Predict test set
```{r}
pred_test_mars_earth <- predict(model_mars_earth, ds_all_dummies_prpr_test)
pred_test_mars_earth <- as.vector(pred_test_mars_earth)
```



## MARS - CARET

### Train Model

```{r}
indx <- createFolds(ds_all_dummies_prpr_train$SalePrice, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = indx)

model_mars_caret <-
  train(
    x = ds_all_dummies_prpr_train[, -c("SalePrice")],
    y = ds_all_dummies_prpr_train$SalePrice,
    method = "earth",
    preProcess = c("BoxCox", "center", "scale"),
    # tuneGrid = expand.grid(degree = 1, nprune = 2:38),
    trControl = ctrl
  )

pred_train_mars_caret <- as.vector(predict(model_mars_caret, ds_all_dummies_prpr_train[, -c("SalePrice", "predicted_pls")]))

model_mars_caret
```

```{r}
summary(model_mars_caret)
```

```{r}
plot(model_mars_caret)
```

### Plot Observed vs Prediction
```{r}
plot(x = ds_all_dummies_prpr_train$SalePrice, y = pred_train_mars_caret)
abline(0,1)
```


### Plot Prediction vs Residuals

```{r}
plot(
  x = pred_train_mars_caret,
  y = (
    pred_train_mars_caret - ds_all_dummies_prpr_train$SalePrice
  )
)
```


### RMSE

```{r}
rmse_mars_caret <- round(sqrt(mean((pred_train_mars_caret - ds_all_dummies_prpr_train$SalePrice)^2)), 0)
rmse_mars_caret
```

RMSE = 24051

### Predict test set
```{r}
pred_test_mars_caret <- as.vector(predict(model_mars_caret, ds_all_dummies_prpr_test))
```


```{r}
marsImp <- varImp(model_mars_caret)
plot(marsImp)
```


# Conclusion on Fast vs Slow

The fast version shows an RMSE of 24k while the more thorough version results in an RMSE of 21.6k; a big difference. 

However, if I want to analyze as many different ML competitions as possible, I should rather go ahead with the fast version. Otherwise the small improvements take up too much time. 


